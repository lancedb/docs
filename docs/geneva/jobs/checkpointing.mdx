---
title: Checkpointing in Geneva
sidebarTitle: Checkpointing
description: Understand how Geneva checkpointing makes distributed jobs restartable, incremental, and idempotent.
icon: arrows-rotate
keywords: ["checkpointing", "geneva", "backfill", "materialized views"]
---

This page describes the current checkpoint mechanism used by Geneva's Ray-based pipelines (backfill and materialized view refresh).

**Audience:** Contributors working on Ray pipeline execution, fault tolerance, and performance.

**See also:**
- [Materialized views](/geneva/jobs/materialized-views/)
- [Distributed job performance](/geneva/jobs/performance/)

## Why checkpointing exists

Checkpointing makes long-running distributed jobs idempotent and restartable:

- **Fault tolerance**: if a worker or actor dies, we can resume without recomputing completed work.
- **Deduplication**: avoid redoing work across retries and incremental refreshes.
- **Incremental execution**: skip already computed fragments and batches during re-runs.

At a high level, the pipeline produces batch checkpoints (RecordBatches) and the writer produces fragment data-file checkpoints (references to staged Lance files). A commit then atomically installs those files into the destination table.

## Core abstractions

### CheckpointStore

All checkpoints are stored in a `CheckpointStore`, a key-value store of `pyarrow.RecordBatch`:

- **Interface**: `src/geneva/checkpoint.py::CheckpointStore`
- **Primary implementation**: `LanceCheckpointStore`, which stores each key as a `{key}.lance` file under a root directory (via `lance.file.LanceFileSession`).

Important constraint: keys become file paths in `LanceCheckpointStore`, so they must not contain dangerous path segments. Geneva avoids this by hashing user-provided strings (URI, where clause) inside key prefixes.

### ReadTask and MapTask

Checkpoint keys are generated by `MapTask` (not by `ReadTask`):

- `ReadTask` (`ScanTask`, `CopyTask`, etc.) defines what to read and its destination fragment or range:
  - `dest_frag_id()`
  - `dest_offset()`
  - `num_rows()`
- `MapTask` (`BackfillUDFTask`, `CopyTableTask`, etc.) defines what to compute and how to build checkpoint keys:
  - `checkpoint_prefix(...)`
  - `checkpoint_key(dataset_uri, dataset_version, frag_id, start, end, where)`

## Checkpoint key formats

Geneva uses stable, structured keys (see `src/geneva/checkpoint_utils.py`).

### Prefix

A prefix encodes the logical identity of work:

- UDF or view name
- UDF version (changes when code changes)
- Output column label
- `where` hash (so different partial backfills do not collide)
- Dataset URI hash (so different tables do not collide)
- Source files hash (hash of source data files for the input columns, when available)

Example prefix shape:

```text
udf-{name}_ver-{version}_col-{column}_where-{md5(where)}_uri-{md5(uri)}_srcfiles-{md5(sorted input data files)}
```

Notes:
- The dataset version is intentionally not part of new checkpoint prefixes. Using dataset versions caused checkpoints to invalidate after any write, even when inputs were unchanged.
- The source files hash is derived from the input columns' source data files for the fragment. This tracks changes in input files without tying identity to dataset versions.

### Per-batch checkpoint key (fragment range)

The canonical unit of checkpointed work is a fragment-local range `[start, end)`:

```text
{prefix}_frag-{frag_id}_range-{start}-{end}
```

Key points:
- `frag_id` is the destination fragment ID.
- `start` and `end` are fragment-local offsets in the same domain as `ScanTask.offset` and `ScanTask.limit` (the domain used by `plan_read`).
- `end` is exclusive.

### Fragment data-file checkpoint key (dedupe key)

Once a fragment has been fully written, the pipeline records a fragment-level checkpoint that points at the staged `.lance` data file.

This key is computed by `src/geneva/runners/ray/pipeline.py::_get_fragment_dedupe_key(...)` and stores:

```json
{
  "file": "<staged_data_file_path>",
  "src_data_files": "[...sorted source data files...]",
  "output_field_ids": "[...sorted output field ids...]"
}
```

This lets the next run skip recomputing that fragment while still including it in the final commit.

For materialized views, the checkpoint may also store `src_data_files` (the union of source data file paths for the inputs that contributed to the destination fragment). This allows refresh logic to detect input changes even if row IDs are stable.

### Legacy keys

Older versions used different checkpoint naming. The current code keeps compatibility by:

- Listing keys by prefix and parsing `_frag-{id}_range-{start}-{end}` when available.
- Falling back to legacy fragment keys (see `src/geneva/apply/__init__.py::_legacy_fragment_dedupe_key`).
- Reconstructing per-batch checkpoints if a legacy task-level checkpoint exists.

## How checkpoints are produced (applier stage)

Ray `ApplierActor` runs a `CheckpointingApplier` over `ReadTask` objects. For each `ReadTask`, the applier:

1. Produces batches (with `_rowaddr` present for writers).
2. Applies the `MapTask` (UDFs or copy transforms).
3. Stores each resulting batch in the `CheckpointStore` under a per-batch checkpoint key.
4. Returns a `MapBatchCheckpoint` object containing:
   - `checkpoint_key`
   - `offset` (fragment-local)
   - `num_rows` (materialized rows in the stored batch)
   - `span` (how far the checkpoint advances in the planner's offset domain)
   - `udf_rows` (rows actually processed by UDFs)

### Range and span semantics

A stored RecordBatch can have fewer rows than the checkpoint coverage. This happens when:

- The input has deletes (logical rows < physical rows), or
- The materialized output is sparse and the applier uses `_rowaddr` to indicate true physical placement.

To keep planning and ordering correct, the applier encodes the intended coverage in the checkpoint key's `_range-{start}-{end}` and in `MapBatchCheckpoint.span`.

## Adaptive checkpoint sizing

Adaptive sizing lets the applier start with small checkpoints (faster proof-of-life and more frequent progress) and then grow checkpoint sizes as it observes runtime throughput, while never exceeding safe bounds.

### How it works

The applier wraps each `ReadTask` with `AdaptiveReadTask` and uses `AdaptiveCheckpointSizer`:

- The underlying read still fetches max-size batches (bounded by `checkpoint_size` or `map_task.batch_size()`).
- Each max batch is sliced into smaller checkpoints based on the adaptive sizer's current size.
- After each emitted checkpoint, the applier records elapsed duration and row count, which updates the sizer for subsequent batches.

This means:
- Planning still uses the same `task_size` and `checkpoint_size` for estimating work.
- Actual checkpoint spans can be smaller than the planned `checkpoint_size` once adaptive sizing is enabled.

### Bounds and defaults

Adaptive sizing is always clamped to bounds:

- `max_checkpoint_size`: upper bound. Defaults to the job's checkpoint size (`map_task.batch_size()`), and is capped to that value if a larger max is provided.
- `min_checkpoint_size`: lower bound. Defaults to `1`.

When `min_checkpoint_size == max_checkpoint_size`, adaptive sizing is effectively disabled (fixed-size checkpoints).

### Override points

You can override adaptive bounds at two levels:

1. UDF definition via `@udf(..., min_checkpoint_size=..., max_checkpoint_size=...)`
2. Backfill call via `table.backfill(..., min_checkpoint_size=..., max_checkpoint_size=...)`

Backfill-level values take precedence over UDF defaults.

### Test stability

Many existing tests assume one checkpoint per plan (for example, `len(results) == 1`). To keep those tests stable:

- Specify `min_checkpoint_size` and `max_checkpoint_size` to the same value (for example, `DEFAULT_CHECKPOINT_ROWS`), or
- Set them on the test UDF or backfill call as needed.

## How checkpoints are used (planning stage)

Planning is done by `plan_read` (see `src/geneva/apply/__init__.py`). It:

1. Enumerates destination fragments.
2. Checks for a fragment data-file checkpoint (fragment-level dedupe key).
   - If present, it skips scheduling compute tasks for the fragment and collects an existing `DataFile` reference so the fragment can be included in commit.
3. Otherwise, it lists per-batch checkpoint keys, parses their `_range-` suffixes, merges covered ranges, and computes missing ranges.
   - Parsers match the trailing `_frag-{id}_range-{start}-{end}` suffix (splitting from the right where needed), so UDF or view names containing substrings like `_range-` do not cause conflicts.
4. Emits `ScanTask` objects for only the missing ranges.

Result: retries and incremental refresh can be O(missing work) rather than O(total work).

### Output data-file validation (rebackfill safety)

When deciding whether a fragment-level checkpoint can be reused, we validate the output column identity using field IDs stored in the checkpoint. This guards against cases like a column being dropped and re-added:

- The input source files (and thus `src_files_hash`) can remain unchanged.
- The output column data files change (new file UUIDs).
- The checkpointed data file was written with the old field IDs.

Lance data files are tied to specific field IDs. A `DataReplacementOperation` cannot use a file written for field-id X to populate a column expecting field-id Y.

Per-batch checkpoints (containing computed Arrow data) remain valid since they are keyed by input hash, not output field IDs. A future optimization could re-serialize cached results to new files with correct field IDs, avoiding UDF recomputation entirely.

If a fragment-level checkpoint is present but the output field IDs no longer match, the planner ignores any per-range checkpoints for that fragment and re-schedules work.

## How checkpoints are consumed (writer stage)

### Ordering and completeness

Fragment writers consume `(offset, checkpoint_key)` pairs through a Ray queue. The writer must reassemble batches in monotonic offset order, even if applier results arrive out of order. This is done with `SequenceQueue`.

Critical detail for deletes:

- Writers expect offsets in the logical-row domain (`frag.count_rows()`), not `frag.physical_rows`.
- Physical layout is restored later using `_rowaddr`.

This is why the writer buffers until it has advanced through `num_logical_rows`, and then aligns to physical layout.

### Physical layout alignment

After reordering, the writer calls `_align_batches_to_physical_layout(...)` to ensure the output fragment has dense physical row addresses:

- Intra-batch gaps are filled (`_fill_rowaddr_gaps`).
- Inter-batch gaps are filled by inserting null rows for missing `_rowaddr` ranges.

This is the step that makes materialized views with filters and backfills with deletes produce a valid physical fragment file.

### Column filtering

Before writing, batches are filtered to the destination schema columns (`_filter_columns_to_schema`) so writers do not accidentally persist internal or extra columns.

## Commit and reuse

The pipeline collects `DataFile` objects from fragment writers and commits them using:

- `lance.LanceOperation.DataReplacement`
- `lance.LanceDataset.commit(..., read_version=...)`

Fragments that were skipped due to fragment-level checkpoints are included as well, so the final table version is consistent.

## Failure modes and how checkpointing prevents them

### Worker or actor failures

- Applier failures: retries can load cached checkpoints and avoid recomputation.
- Writer failures: writers can be restarted; cached tasks and checkpoint keys can be replayed.

### Empty work units

If a `ReadTask` produces no batches (for example, everything filtered out), the applier still persists completion checkpoints (null-filled batches) so the task is idempotent and the job can finish.

### Common hang symptom: logical vs physical row mismatch

If any stage mixes up:

- `frag.count_rows()` (logical rows, after deletes or filters)
- `frag.physical_rows` (physical rows)

It can lead to a writer waiting forever for offsets that will never be produced. Writers should advance in the planner's offset domain (logical rows) and only use `_rowaddr` to restore physical layout.

## Debugging tips

- **List keys**: `CheckpointStore.list_keys(prefix)` is the source of truth and keys are human-readable.
- **Look for `_range-` keys**: they indicate per-batch checkpoints.
- **Look for fragment dedupe keys**: they indicate a fragment has a staged data file that can be reused.
- **If a job hangs**: inspect whether the writer is waiting for offsets beyond `frag.count_rows()` (logical rows) or whether checkpoint ranges have gaps.
