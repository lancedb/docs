---
title: Execution Contexts
sidebarTitle: Contexts
description: Learn how Geneva automatically packages and deploys your Python execution environment to worker nodes for distributed execution.
icon: circle-nodes
---

Geneva automatically packages and deploys your Python execution environment to its worker nodes. This ensures that distributed execution occurs in the same environment and dependencies as your prototype.

We currently support one processing backend: **Ray**. There are 3 ways to connect to a Ray cluster: 

1. Local Ray
2. KubeRay: create a cluster on demand in your Kubernetes cluster.
3. Existing Ray Cluster

## Ray Clusters

### Local Ray

To execute jobs without an external Ray cluster, you can just trigger the `Table.backfill` method. This will auto-create a Ray cluster on your machine. Because it's on your laptop/desktop, this is only suitable for prototyping on small datasets. But it is the easiest way to get started. Simply define the UDF, add a column, and trigger the job:

<CodeGroup>
```python Python icon="python"
@udf
def filename_len(filename: str) -> int:
    return len(filename)

tbl.add_columns({"filename_len": filename_len})
tbl.backfill("filename_len")
```
</CodeGroup>

Geneva will package up your local environment and send it to each worker node, so they'll have access to all the same dependencies as if you ran a simple Python script yourself.

### KubeRay

If you have a Kubernetes cluster with KubeRay, you can connect to it by defining a Geneva cluster. A Geneva cluster represents the resource needs, Docker images, and other Ray configurations necessary to run your job. Make sure that the resources requested will fit inside the Kubernetes cluster you're connecting to. Here is an example Geneva cluster definition:

<CodeGroup>
```python Python icon="python"
import sys
import ray
import geneva
from geneva.cluster.builder import GenevaClusterBuilder
from geneva.cluster import K8sConfigMethod
from geneva.runners.ray.raycluster import get_ray_image

db = geneva.connect("s3://geneva-integ-test-devland-us-east-1/dantasse-geneva-demo")

ray_version = ray.__version__
python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
cluster_name = "my-geneva-cluster" # lowercase, numbers, hyphens only
service_account = "my_k8s_service_account" # k8s service account bound geneva runs as
k8s_namespace = "geneva"  # k8s namespace

cluster = (
    GenevaClusterBuilder()
        .name(cluster_name)
        .namespace(k8s_namespace)
        .portforwarding(True) # required for kuberay to expose ray ports
        .aws_config(region="us-east-1") # only required if using AWS
        .config_method(K8sConfigMethod.LOCAL) # Load k8s config from `~/.kube.config`
        # (other options include EKS_AUTH to load from AWS EKS, or IN_CLUSTER to load the
        # config when running inside a pod in the cluster)
        .head_group(
            service_account=service_account,
            image=get_ray_image(ray_version, python_version),
            cpus=2,
            node_selector={"geneva.lancedb.com/ray-head":""}, # k8s label required for head in your cluster
        )
        .add_cpu_worker_group(
            cpus=4,
            memory="8Gi",
            service_account=service_account,
            image=get_ray_image(ray_version, python_version),
            node_selector={"geneva.lancedb.com/ray-worker-cpu":""}, # k8s label for cpu worker in your cluster
        )
        .add_gpu_worker_group(
            cpus=2,
            memory="8Gi",
            gpus=1,
            service_account=service_account,
            image=get_ray_image(ray_version, python_version, gpu=True), # Note the GPU image here
            node_selector={"geneva.lancedb.com/ray-worker-gpu":""}, # k8s label for gpu worker in your cluster
        )
        .build()
)

db.define_cluster(cluster_name, cluster)
# define_cluster stores the cluster metadata in persistent storage. The Cluster can then be referenced by name and provisioned when creating an execution context.
```
See [the API docs](https://lancedb.github.io/geneva/api/cluster/) for all the parameters GenevaClusterBuilder can use.

</CodeGroup>

### External Ray cluster
If you already have a Ray cluster (specified through some other means), Geneva can execute jobs against it too. You do so by defining a Geneva cluster which has the address of the cluster. Here's an example:

<CodeGroup>
```python Python icon="python"
import geneva
from geneva.cluster.builder import GenevaClusterBuilder
from geneva.cluster import GenevaClusterType

db = geneva.connect(my_db_uri)
cluster_name = "my-geneva-external-cluster"

cluster = (
    GenevaClusterBuilder()
    .name(cluster_name)
    .cluster_type(GenevaClusterType.EXTERNAL_RAY)
    .ray_address("ray://my_ip:my_port")
    .portforwarding(False) # This must be False when using an external Ray cluster
    .build()
)
db.define_cluster(cluster_name, cluster)

```
</CodeGroup>

## Dependencies

Most UDFs require some dependencies: helper libraries like `pillow` for image processing, pre-trained models like `open-clip` to calculate embeddings, or even small config files. We have two ways to get them to workers:

1. Use defaults
2. Define a manifest

### Use Defaults
By default, LanceDB packages your local environment and sends it to Ray workers. This includes your local Python `site-packages` (defined by `site.getsitepackages()`) and either the current workspace root (if you're in a python repo) or the current working directory (if you're not). If you don't explicitly define a manifest, this is what will happen.

### Define a Manifest

Sometimes you need more control over what the workers get access to. For example:
- you might need to include files from another directory, or another python package
- you might not want to send all your local dependencies (if your repo has lots of dependencies but your UDF will only need a few)
- you might need packages to be built separately for the worker's architecture (for example, you can't build `pyarrow` on a Mac and run it on a Linux Ray worker).
- you might want to reuse dependencies between two backfill jobs, so you know they are running with the same environment.

For these use cases, you can define a Manifest. Calling `define_manifest()` packages files in the local environment and stores the Manifest metadata and files in persistent storage. The Manifest can then be referenced by name, shared, and reused.

<CodeGroup>
```python Python icon="python"
from geneva.manifest.builder import GenevaManifestBuilder

db = geneva.connect(my_db_uri)

manifest_name="dev-manifest"
manifest = (
    GenevaManifestBuilder()
        .name(manifest_name)
        .skip_site_packages(False)
        .pip(["lancedb", "numpy"])
        .py_modules(["my_module"])
    ).build()
)

db.define_manifest(manifest_name, manifest)
```
</CodeGroup>

This manifest contains a couple ways to get files and resources to the Ray workers:
- local environment: Everything in your local environment, including your local working directory and python `site_packages`, will be zipped and sent to workers.
  - you can see these zip files by setting `.local_zip_output_dir(path)` in the builder, or set `.delete_local_zips(True)` if you don't care
  - you can set `skip_site_packages=True` if you don't want to upload your entire local `site_packages`. This is useful, for example, for the `pyarrow` or many-libraries cases we mentioned earlier. In that case, you will probably need `pip` and `py_modules`:
- `pip` and `py_modules`: packages that you want to be installed, but are not installed locally. These are passed in to Ray's [RuntimeEnv](https://docs.ray.io/en/latest/ray-core/api/doc/ray.runtime_env.RuntimeEnv.html#ray-runtime-env-runtimeenv), which has more details about how they are included. In short, `pip` is a list of packages that you want it to install from pip, and `py_modules` is a list of local or remote zip files that Ray unzips and adds to workers' PYTHONPATHs.

## Putting it all together: Execution Contexts

An execution context represents the concrete execution environment (Cluster and Manifest) used to execute a distributed job.

Calling `context` will enter a context manager that will provision an execution cluster and execute the Job using the Cluster and Manifest definitions provided. Because you've already defined the cluster and manifest, you can just refernece them by name. Note that providing a manifest is optional. Once completed, the context manager will automatically de-provision the cluster.

<CodeGroup>
```python Python icon="python"
db = geneva.connect(my_db_uri)
tbl = db.get_table("my_table")

# Providing a manifest is optional; if not provided, it will work as described in "Use defaults" above.
with db.context(cluster=cluster_name, manifest=manifest_name):
    tbl.backfill("embedding")
```
</CodeGroup>

In a notebook environment, you can manually enter and exit the context manager in multiple steps like so: 

<CodeGroup>
```python Python icon="python"
ctx = db.context(cluster=cluster_name, manifest=manifest_name)
ctx.__enter()__

# ... do stuff

ctx.__exit__(None,None,None)
```
</CodeGroup>
