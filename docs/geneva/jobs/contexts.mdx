---
title: Execution Contexts
sidebarTitle: Contexts
description: Learn how Geneva automatically packages and deploys your Python execution environment to worker nodes for distributed execution.
icon: circle-nodes
---

Geneva automatically packages and deploys your Python execution environment to its worker nodes. This ensures that distributed execution occurs in the same environment and dependencies as your prototype.

We currently support one processing backend: **Ray**. Geneva jobs can be deployed on a Kubernetes cluster on demand or on an existing Ray cluster.

<Card icon="lightbulb">
If you are using a remote Ray cluster, you will need to have the notebook or script that code is packaged on running the same CPU architecture / OS. By default, Ray clusters are run in Linux. If you host a Jupyter service on a Mac, Geneva will attempt to deploy Mac shared libraries to a Linux cluster and result in "Module not found" errors. You can instead use a hosted Jupyter notebook, or host your Jupyter or Python environment on a Linux VM or container.
</Card>

## Ray Auto Connect

To execute jobs without an external Ray cluster, you can just trigger the `Table.backfill` method. This will auto-create a local Ray cluster and is only suitable for prototyping on small datasets. Simply define the UDF, add a column, and trigger the job:

<CodeGroup>
```python Python icon="python"
@udf
def filename_len(filename: str) -> int:
    return len(filename)

tbl.add_columns({"filename_len": filename_len})
tbl.backfill("filename_len")
```
</CodeGroup>

## Existing Ray Cluster

Geneva can execute jobs against an existing Ray cluster. You can define a RayCluster by specifying the address of the cluster and packages needed on your workers.

This approach makes it easy to tailor resource requirements to your particular UDFs.

You can then wrap your table backfill call with the RayCluster context.

<CodeGroup>
```python Python icon="python"
from geneva.runners.ray.raycluster import _HeadGroupSpec, _WorkerGroupSpec
from geneva.runners.ray._mgr import ray_cluster

geneva.connect(my_db_uri)

with ray_cluster(
        addr = "ray-head:10001"  # replace ray head with the address of your ray head node
        skip_site_packages=False, # optionally skip shipping python site packages if already in image
        use_portforwarding=False,  # Must be False when byo ray cluster
        pip=[], # list of pip package or urls to install on each image.
    ):

    tbl.backfill("xy_product")
```
</CodeGroup>

> **Note**: If your Ray cluster is managed by KubeRay, you'll need to setup kubectl port forwarding setup so Geneva can connect.

For more interactive usage, you can use this pattern:

<CodeGroup>
```python Python icon="python"
# this is a k8s pod spec.
raycluster = ray_cluster(...)
raycluster.__enter__() # equivalent of ray.init()

#  trigger the backfill on column "filename_len" 
tbl.backfill("filename_len") 

raycluster.__exit__(None, None, None)
```
</CodeGroup>

## Ray on Kubernetes

Geneva uses KubeRay to deploy Ray on Kubernetes. To do so, you need to create an ExecutionContext, which needs two things:

- a Geneva Cluster. This includes the name, resource demands, Docker images, and other Ray configurations
- a Geneva Manifest. This includes the python packages and any local files that each worker will need

These Clusters and Manifests can be persisted and shared between different users.

### Define a Cluster

A Geneva Cluster represents the resource needs, Docker images, and other Ray configurations necessary to run your job. Make sure that the resources requested will fit inside the kuberay cluster you're connecting to.

<CodeGroup>
```python Python icon="python"
import sys
import ray
from geneva.cluster.builder import GenevaClusterBuilder
from geneva.runners.ray.raycluster import get_ray_image

db = geneva.connect(my_db_uri)
ray_version = ray.__version__
python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
cluster_name = "my_geneva_cluster"
service_account = "my_k8s_service_account" # k8s service account bound geneva runs as

cluster = GenevaClusterBuilder()
        .name(cluster_name)
        .namespace(k8s_namespace)
        .portforwarding(True) # required for kuberay to expose ray ports
        .aws_config(region="us-east-1") # only required if using AWS
        .k8s_config_method(K8sConfigMethod.LOCAL) # Load k8s config from `~/.kube.config`
        # (other options include EKS_AUTH to load from AWS EKS, or IN_CLUSTER to load the
        # config when running inside a pod in the cluster)
        .head_group(
            service_account=service_account,
            image=get_ray_image(ray_version, python_version)
            cpus=2,
            node_selector={"geneva.lancedb.com/ray-head":""}, # k8s label required for head in your cluster
        )
        .add_cpu_worker_group(
            cpus=4,
            memory="8Gi",
            service_account=service_account,
            image=get_ray_image(ray_version, python_version)
            node_selector={"geneva.lancedb.com/ray-worker-cpu":""}, # k8s label for cpu worker in your cluster
        )
        .add_gpu_worker_group(
            cpus=2,
            memory="8Gi",
            gpus=1,
            service_account=service_account,
            image=get_ray_image(ray_version, python_version, gpu=True) # Note the GPU image here
            node_selector={"geneva.lancedb.com/ray-worker-gpu":""}, # k8s label for gpu worker in your cluster
        )
        .build()

db.define_cluster("my_geneva_cluster", cluster)
# define_cluster stores the cluster metadata in persistent storage. The Cluster can then be referenced by name and provisioned when creating an Execution Context.
```
</CodeGroup>

### Define a Manifest

A Geneva Manifest represents the files and dependencies used in the execution environment. Calling `define_manifest()` packages files in the local environment and stores the Manifest metadata and files in persistent storage.
The Manifest can then be referenced by name when creating an Execution Context. Persistent Manifests allow for deterministic execution environments that can be shared and reused.

<CodeGroup>
```python Python icon="python"
from geneva.manifest.builder import GenevaManifestBuilder

db = geneva.connect(my_db_uri)

manifest_name="dev-manifest"
manifest = (
    GenevaManifestBuilder()
        .name(manifest_name)
        .skip_site_packages(False)
        .pip(["lancedb", "numpy"])
        .py_modules(["my_module"])
    ).build()
)

db.define_manifest(manifest_name, manifest)
```
</CodeGroup>

This manifest contains a couple ways to get files and resources to the Ray workers:
- local environment: Everything in your local environment, including your local working directory and python `site_packages`, will be zipped and sent to workers.
  - you can see these zip files by setting `.local_zip_output_dir(path)` in the builder, or set `.delete_local_zips(True)` if you don't care
  - you can set `skip_site_packages=True` if you don't want to upload your local `site_packages`. This is useful, for example, if youâ€™re developing on an ARM64 machine (e.g., Apple Silicon Macs) and want to avoid sending ARM64 prebuilt packages to x86-64 Ray nodes. In that case, you will probably need `pip` and `py_modules`:
- `pip` and `py_modules`: packages that you want to be installed, but are not installed locally. These are passed in to Ray's [RuntimeEnv](https://docs.ray.io/en/latest/ray-core/api/doc/ray.runtime_env.RuntimeEnv.html#ray-runtime-env-runtimeenv), which has more details about how they are included. In short, `pip` is a list of packages that you want it to install from pip, and `py_modules` is a list of local or remote zip files that Ray unzips and adds to workers' PYTHONPATHs.

### Create an Execution Context

An Execution Context represents the concrete execution environment used to execute a distributed Job.

Calling `context` will enter a context manager that will provision an execution cluster and execute the Job using the Cluster and Manifest definitions provided. Once completed, the context manager will automatically de-provision the cluster.

<CodeGroup>
```python Python icon="python"
db = geneva.connect(my_db_uri)
tbl = db.get_table("my_table")

with db.context(cluster=cluster_name, manifest=manifest_name):
    tbl.backfill("embedding")
```
</CodeGroup>

In a notebook environment, you can manually enter and exit the context manager in multiple steps like so: 

<CodeGroup>
```python Python icon="python"
ctx = db.context(cluster=cluster_name, manifest=manifest_name)
ctx.__enter()__

# ... do stuff

ctx.__exit__(None,None,None)
```
</CodeGroup>