---
title: "GPU-Powered Vector Indexing"
sidebarTitle: "GPU indexing"
description: "Accelerate IVF and HNSW index builds with GPU acceleration in LanceDB."
icon: "/static/assets/logo/lance-logo-gray.svg"
---
import { PyGpuIndexCuda as GpuIndexCuda, PyGpuIndexMps as GpuIndexMps } from '/snippets/indexing.mdx';

With LanceDB's GPU-powered vector indexing you can index very large datasets in far less time
than you could with the default CPU-based indexing. In our tests, LanceDB
is capable of indexing billions of rows in under four hours on a 1-8 GPU cluster.

<Info>
**Automatic GPU indexing**
<Badge color="red">Enterprise-only</Badge>

Automatic GPU Indexing is currently only available in [LanceDB Enterprise](/enterprise/).
Please [contact us](mailto:contact@lancedb.com) to enable this feature for your deployment.

The vector index is created when you call `create_index`. The backend will use GPU resources
to build either the IVF or HNSW indexes. The system automatically selects the optimal GPU
configuration based on your data size and available hardware.

This process is also asynchronous by default, but you can use `wait_for_index` to convert it
into a synchronous process by waiting until the index is built.

</Info>

## Manual GPU indexing in LanceDB OSS

You can use the Python SDK to manually create the `IVF_PQ` index on a GPU. You'll need
[PyTorch>2.0](https://pytorch.org/). Note that GPU-based indexing is currently only
supported by the synchronous SDK in LanceDB OSS.

Specify the values `cuda` or `mps` (on Apple Silicon) for the `accelerator` parameter
to enable GPU training on your device.

### GPU indexing on Linux

<CodeGroup>
    <CodeBlock filename="Python" language="Python" icon="python">
    {GpuIndexCuda}
    </CodeBlock>
</CodeGroup>

### GPU indexing on macOS (Apple Silicon)

<CodeGroup>
    <CodeBlock filename="Python" language="Python" icon="python">
    {GpuIndexMps}
    </CodeBlock>
</CodeGroup>

## Performance considerations

- GPU memory usage scales with `num_partitions` and vector dimensions
- For optimal performance, ensure GPU memory exceeds dataset size
- Batch size is automatically tuned based on available GPU memory
- Indexing speed improves with larger batch sizes

## Troubleshooting

If you encounter the error `AssertionError: Torch not compiled with CUDA enabled`,
you need to [install PyTorch with CUDA support](https://pytorch.org/get-started/locally/).

