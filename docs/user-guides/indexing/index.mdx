---
title: "Indexing Data in LanceDB"
sidebarTitle: "Indexing Data"
description: "Optimize search performance in LanceDB using vector, full-text, and scalar indexes."
weight: 102
aliases:
  - "/docs/concepts/indexing/"
  - "/docs/concepts/indexing"
---

Embeddings for a dataset become searchable through an **index**, a data structure that stores embeddings for efficient scans and lookups.

LanceDB provides a comprehensive suite of indexing strategies:

- **Vector Index**: Optimized for high-dimensional data (images, audio, or text embeddings)
- **Full-Text Search Index**: Enables fast keyword-based searches
- **Scalar Index**: Accelerates filtering and sorting of structured numeric or categorical data

<Tip title="Working with scalar indexes">
Scalar indexes serve as a foundational optimization layer. They can be combined with vector search (prefilter or post-filter), full-text search, SQL scans, or key-value lookups for rapid primary-key retrievals.
</Tip>

## Supported Index Types

LanceDB provides a comprehensive suite of indexing strategies for different data types and use cases:

| Index | Use Case | Description |
| :--------- | :------- | :---------- |
| `HNSW` (Vector) | High recall and low latency vector searches. Ideal for applications requiring fast approximate nearest neighbor queries with high accuracy. | Hierarchical Navigable Small World—a graph-based approximate nearest neighbor algorithm.<br />Distance metrics: `l2` `cosine` `dot`<br />Quantizations: `PQ` `SQ`|
| `IVF` (Vector) | Large-scale vector search with configurable accuracy/speed trade-offs. Supports binary vectors with hamming distance. | Inverted File Index—a partition-based approximate nearest neighbor algorithm that groups similar vectors into partitions for efficient search.<br />Distance metrics: `l2` `cosine` `dot` `hamming`<br />Quantizations: `None/Flat` `PQ` `SQ` `RQ`|
| `BTree` (Scalar) | Numeric, temporal, and string columns with mostly distinct values. Best for highly selective queries on columns with many unique values. | Sorted index storing sorted copies of scalar columns with block headers in a btree cache. Header entries map to blocks of rows (4096 rows per block) for efficient disk reads. |
| `Bitmap` (Scalar) | Low-cardinality columns with few thousand or fewer distinct values. Accelerates equality and range filters. | Stores a bitmap for each distinct value in the column, with one bit per row indicating presence. Memory-efficient for low-cardinality data. |
| `LabelList` (Scalar) | List columns (e.g., tags, categories, keywords) requiring array containment queries. | Scalar index for `List<T>` columns using an underlying bitmap index structure to enable fast array membership lookups. |
| `FTS` (Full-text) | String columns (e.g., title, description, content) requiring keyword-based search with BM25 ranking. | Full-text search index using BM25 ranking algorithm. Tokenizes text with configurable tokenization, stemming, stop word removal, and language-specific processing. |

<Note>
TypeScript currently doesn't support `IvfSq` (IVF with Scalar Quantization).
</Note>

### Quantization Types

Vector indexes can use different quantization methods to compress vectors and improve search performance:

| Quantization | Use Case | Description |
| :----------- | :------- | :---------- |
| `PQ` (Product Quantization) | Default choice for most vector search scenarios. Use when you need to balance index size and recall. | Divides vectors into subvectors and quantizes each subvector independently. Provides a good balance between compression ratio and search accuracy. |
| `SQ` (Scalar Quantization) | Use when you need faster indexing or when vector dimensions have consistent value ranges. | Quantizes each dimension independently. Simpler than PQ but typically provides less compression. |
| `RQ` (RabitQ Quantization) | Use when you need maximum compression or have specific per-dimension requirements. | Per-dimension quantization using a RabitQ codebook. Provides fine-grained control over compression per dimension. |
| `None/Flat` | Use for binary vectors (with `hamming` distance) or when you need maximum recall and have sufficient storage. | No quantization—stores raw vectors. Provides the highest accuracy but requires more storage and memory. |

## Understanding the IVF-PQ Index

An Approximate Nearest Neighbor (ANN) index re-represents data so that searches become faster (with a slight accuracy trade-off vs. brute-force search). LanceDB's disk-based IVF-PQ index is a variant of the Inverted File Index (IVF) that uses Product Quantization (PQ) to compress embeddings.

LanceDB is built on top of [Lance](https://github.com/lancedb/lance), an open-source columnar format designed for performant ML workloads and fast random access. Because of this foundation, LanceDB adopts a primarily *disk-based* indexing philosophy.

## IVF-PQ

IVF-PQ combines inverted file partitions with product quantization. The implementation in LanceDB exposes parameters you can fine-tune for index size, throughput, latency, and recall.

### Product Quantization

Quantization compresses embeddings to speed up search. Product quantization (PQ) divides a high-dimensional vector into equal-sized subvectors. Each subvector maps to the nearest centroid in its PQ codebook.

![](/assets/docs/ivfpq_pq_desc.png)

Quantization is *lossy*, so reconstructed vectors differ slightly from the originals. This trades accuracy for memory.

<Note title="Effect of quantization">
Original: `128 × 32 = 4096` bits  
Quantized: `4 × 8 = 32` bits  
This yields a **128×** reduction in memory requirements per vector.
</Note>

### Inverted File Index (IVF) Implementation

PQ reduces index size, while IVF accelerates search by narrowing the candidate space. IVF partitions the PQ vector space into *Voronoi cells* by running K-means and using the centroids as region seeds.

![](/assets/docs/ivfpq_ivf_desc.webp)

During query time, a vector may lie near multiple cells. The `nprobe` parameter controls how many cells to search (higher `nprobe` improves accuracy but increases latency).

![](/assets/docs/ivfpq_query_vector.webp)

## HNSW Index Implementation

Hierarchical Navigable Small Worlds (HNSW) is one of the most accurate and fastest ANN algorithms for high-dimensional data.

### Types of ANN Search Algorithms

- **Tree-based**: organize points into a tree.
- **Hash-based**: rely on geometric hashing; good theoretical guarantees but weaker empirical performance.
- **Graph-based**: represent points as a graph. HNSW is graph-based and combines k-NN graphs with skip-list concepts.

### Understanding k-Nearest Neighbor Graphs

Each vector becomes a vertex connected to its k nearest neighbors (k edges). A greedy walk from an entry point toward closer neighbors yields good approximate neighbors. Building k-NN graphs exactly is quadratic, so practical systems use approximate constructions or incremental updates.

One downside: getting good accuracy can require large `k`, increasing index size.

### Hierarchical Navigable Small Worlds (HNSW)

HNSW improves upon k-ANN graphs by:

- Pruning edges with heuristics so each vertex only keeps a small constant number of edges.
- Using multiple layers (like a skip list) and dynamic entry points to speed up search.

Layers contain decreasing fractions of the dataset. Greedy search starts at the top layer, descends layer by layer, and finishes with a bottom-layer search to retrieve multiple neighbors.

