---
title: "Configuring Cloud Storage in LanceDB"
sidebarTitle: "Configuring storage"
description: "Configure LanceDB to use S3, GCS, Azure Blob, and S3-compatible object stores with environment variables or storage options."
icon: "wrench"
---
import {
    PyStorageAzureAccount,
    PyStorageConnectAzure,
    PyStorageConnectGcs,
    PyStorageConnectS3,
    PyStorageConnectTimeout,
    PyStorageGcsServiceAccount,
    PyStorageOptionsProvider,
    PyStorageS3Ddb,
    PyStorageS3Express,
    PyStorageS3Minio,
    PyStorageTableTimeout,
    PyStorageTigrisConnect,
    TsStorageAzureAccount,
    TsStorageConnectAzure,
    TsStorageConnectGcs,
    TsStorageConnectS3,
    TsStorageConnectTimeout,
    TsStorageGcsServiceAccount,
    TsStorageS3Ddb,
    TsStorageS3Express,
    TsStorageS3Minio,
    TsStorageTableTimeout,
    TsStorageTigrisConnect,
} from '/snippets/storage.mdx';

When using LanceDB OSS, you can choose where to store your data. The tradeoffs between storage options are covered in the [storage architecture guide](/storage). This page shows how to configure each backend.

## Object stores

LanceDB supports AWS S3 (and compatible stores), Azure Blob Storage, and Google Cloud Storage. The URI scheme in your `connect` call selects the backend.

<CodeGroup>
    <CodeBlock language="Python" title="AWS S3" icon="python">
    {PyStorageConnectS3}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="AWS S3" icon="square-js">
    {TsStorageConnectS3}
    </CodeBlock>
</CodeGroup>

<CodeGroup>
    <CodeBlock language="Python" title="Google Cloud Storage" icon="python">
    {PyStorageConnectGcs}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Google Cloud Storage" icon="square-js">
    {TsStorageConnectGcs}
    </CodeBlock>
</CodeGroup>

<CodeGroup>
    <CodeBlock language="Python" title="Azure Blob Storage" icon="python">
    {PyStorageConnectAzure}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Azure Blob Storage" icon="square-js">
    {TsStorageConnectAzure}
    </CodeBlock>
</CodeGroup>

### Configuration options

When running inside the target cloud with correct IAM bindings, LanceDB often needs no extra configuration. When running elsewhere, provide credentials via environment variables or `storage_options`.

<CodeGroup>
    <CodeBlock language="Python" title="Set a request timeout" icon="python">
    {PyStorageConnectTimeout}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Set a request timeout" icon="square-js">
    {TsStorageConnectTimeout}
    </CodeBlock>
</CodeGroup>

<Info title="Storage option casing">
Keys are case-insensitive. Use lowercase in `storage_options` and uppercase in environment variables.
</Info>

If you need the option to apply only to a specific table:

<CodeGroup>
    <CodeBlock language="Python" title="Table-level storage options" icon="python">
    {PyStorageTableTimeout}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Table-level storage options" icon="square-js">
    {TsStorageTableTimeout}
    </CodeBlock>
</CodeGroup>

#### General object store options

| Key | Description |
| :-- | :-- |
| `allow_http` | Allow non-TLS connections. Default: `false`. |
| `allow_invalid_certificates` | Skip certificate validation. Default: `false`. |
| `connect_timeout` | Timeout for the connect phase. Default: `5s`. |
| `timeout` | Timeout for the full request. Default: `30s`. |
| `user_agent` | User agent string to send with requests. |
| `proxy_url` | Proxy URL to route requests through. |
| `proxy_ca_certificate` | PEM-formatted CA certificate for proxy connections. |
| `proxy_excludes` | Comma-separated hosts that bypass the proxy (domains or CIDR). |

### Dynamic / expiring credentials (StorageOptionsProvider)

In production, cloud credentials are often **temporary** (for example, AWS STS session tokens, Kubernetes IRSA, or workload identity). In these cases, setting static environment variables or passing a fixed `storage_options` dictionary may stop working once the credentials expire.

LanceDB supports dynamic credential refresh via a **storage options provider**:

- Implement a `StorageOptionsProvider` with a `fetch_storage_options()` method.
- Pass it to `create_table(..., storage_options_provider=...)` or `open_table(..., storage_options_provider=...)`.
- LanceDB will call your provider to obtain up-to-date `storage_options` when it needs to access the underlying object store.

<Note>
This pattern is intended for **refreshing credentials**. Use `storage_options` for static configuration (region, endpoint, timeouts, etc.) and a provider for values that may change (access keys, session tokens, etc.).
</Note>

<CodeGroup>
  <CodeBlock language="Python" title="StorageOptionsProvider example" icon="python">
    {PyStorageOptionsProvider}
  </CodeBlock>
</CodeGroup>

## AWS S3

![](/static/assets/images/storage/aws.jpg)

Set `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and optionally `AWS_SESSION_TOKEN` as environment variables or pass them in `storage_options`. Region is optional for AWS but required for most S3-compatible stores.

Minimum permissions usually include `s3:PutObject`, `s3:GetObject`, `s3:DeleteObject`, `s3:ListBucket`, and `s3:GetBucketLocation` scoped to the relevant bucket/prefix.

### S3-compatible stores

<CodeGroup>
    <CodeBlock language="Python" title="Connect to an S3-compatible endpoint" icon="python">
    {PyStorageS3Minio}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to an S3-compatible endpoint" icon="square-js">
    {TsStorageS3Minio}
    </CodeBlock>
</CodeGroup>

If the endpoint is `http://` (common in local development), also set `ALLOW_HTTP=true` or pass `allow_http=True` in `storage_options`.

### S3 Express

<CodeGroup>
    <CodeBlock language="Python" title="Use an S3 Express One Zone bucket" icon="python">
    {PyStorageS3Express}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Use an S3 Express One Zone bucket" icon="square-js">
    {TsStorageS3Express}
    </CodeBlock>
</CodeGroup>

Consult AWS networking requirements for S3 Express before enabling.

### DynamoDB commit store for concurrent writes

S3 lacks atomic writes. To enable safe concurrent writers, use DynamoDB as a commit store by switching to the `s3+ddb` scheme and specifying the table name.

<CodeGroup>
    <CodeBlock language="Python" title="Enable DynamoDB-backed commits" icon="python">
    {PyStorageS3Ddb}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Enable DynamoDB-backed commits" icon="square-js">
    {TsStorageS3Ddb}
    </CodeBlock>
</CodeGroup>

Create the DynamoDB table with hash key `base_uri` (string) and range key `version` (number). Small provisioned throughput (for example `ReadCapacityUnits=1`, `WriteCapacityUnits=1`) is sufficient for coordination.

<Tip title="Clean up failed multipart uploads">
LanceDB aborts multipart uploads on graceful shutdown, but crashes can leave incomplete uploads. Add an S3 lifecycle rule to delete in-progress uploads after a few days.
</Tip>

## Google Cloud Storage

![](/static/assets/images/storage/gcp.jpg)

Provide credentials via `GOOGLE_SERVICE_ACCOUNT` (path to JSON) or include the path in `storage_options`. GCS defaults to HTTP/1; set `HTTP1_ONLY=false` if you need HTTP/2.

<CodeGroup>
    <CodeBlock language="Python" title="Connect with a service account" icon="python">
    {PyStorageGcsServiceAccount}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect with a service account" icon="square-js">
    {TsStorageGcsServiceAccount}
    </CodeBlock>
</CodeGroup>

## Azure Blob Storage

![](/static/assets/images/storage/azure.jpg)

Set `AZURE_STORAGE_ACCOUNT_NAME` and `AZURE_STORAGE_ACCOUNT_KEY` as environment variables, or pass them via `storage_options`.

<CodeGroup>
    <CodeBlock language="Python" title="Connect to Azure Blob Storage" icon="python">
    {PyStorageAzureAccount}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to Azure Blob Storage" icon="square-js">
    {TsStorageAzureAccount}
    </CodeBlock>
</CodeGroup>

Other supported keys include service principal credentials (`azure_client_id`, `azure_client_secret`, `azure_tenant_id`), SAS tokens, managed identities, and custom endpoints.

## Tigris Object Storage

![](/static/assets/images/storage/tigris.jpg)

Tigris exposes an S3-compatible API. Configure the endpoint and region:

<CodeGroup>
    <CodeBlock language="Python" title="Connect to Tigris Object Storage" icon="python">
    {PyStorageTigrisConnect}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to Tigris Object Storage" icon="square-js">
    {TsStorageTigrisConnect}
    </CodeBlock>
</CodeGroup>

Environment variables `AWS_ENDPOINT=https://t3.storage.dev` and `AWS_DEFAULT_REGION=auto` achieve the same configuration.

