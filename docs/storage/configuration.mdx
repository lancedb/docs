---
title: "Configuring Cloud Storage in LanceDB"
sidebarTitle: "Configuring storage"
description: "Configure LanceDB to use S3, GCS, Azure Blob, and S3-compatible object stores with environment variables or storage options."
icon: "wrench"
---
import {
    PyStorageAzureAccount,
    PyStorageConnectAzure,
    PyStorageConnectGcs,
    PyStorageConnectS3,
    PyStorageConnectTimeout,
    PyStorageProviderRefresh,
    PyStorageGcsServiceAccount,
    PyStorageS3Ddb,
    PyStorageS3Express,
    PyStorageS3Minio,
    PyStorageTableTimeout,
    PyStorageTigrisConnect,
    TsStorageAzureAccount,
    TsStorageConnectAzure,
    TsStorageConnectGcs,
    TsStorageConnectS3,
    TsStorageConnectTimeout,
    TsStorageGcsServiceAccount,
    TsStorageS3Ddb,
    TsStorageS3Express,
    TsStorageS3Minio,
    TsStorageTableTimeout,
    TsStorageTigrisConnect,
} from '/snippets/storage.mdx';

When using LanceDB OSS, you can choose where to store your data. The tradeoffs between storage options are covered in the [storage architecture guide](/storage). This page shows how to configure each backend.

## Object stores

LanceDB supports AWS S3 (and compatible stores), Azure Blob Storage, and Google Cloud Storage. The URI scheme in your `connect` call selects the backend.

<CodeGroup>
    <CodeBlock language="Python" title="AWS S3" icon="python">
    {PyStorageConnectS3}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="AWS S3" icon="square-js">
    {TsStorageConnectS3}
    </CodeBlock>
</CodeGroup>

<CodeGroup>
    <CodeBlock language="Python" title="Google Cloud Storage" icon="python">
    {PyStorageConnectGcs}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Google Cloud Storage" icon="square-js">
    {TsStorageConnectGcs}
    </CodeBlock>
</CodeGroup>

<CodeGroup>
    <CodeBlock language="Python" title="Azure Blob Storage" icon="python">
    {PyStorageConnectAzure}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Azure Blob Storage" icon="square-js">
    {TsStorageConnectAzure}
    </CodeBlock>
</CodeGroup>

### Configuration options

When running inside the target cloud with correct IAM bindings, LanceDB often needs no extra configuration. When running elsewhere, provide credentials via environment variables or `storage_options`.

<CodeGroup>
    <CodeBlock language="Python" title="Set a request timeout" icon="python">
    {PyStorageConnectTimeout}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Set a request timeout" icon="square-js">
    {TsStorageConnectTimeout}
    </CodeBlock>
</CodeGroup>

<Info title="Storage option casing">
Keys are case-insensitive. Use lowercase in `storage_options` and uppercase in environment variables.
</Info>

If you need the option to apply only to a specific table:

<CodeGroup>
    <CodeBlock language="Python" title="Table-level storage options" icon="python">
    {PyStorageTableTimeout}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Table-level storage options" icon="square-js">
    {TsStorageTableTimeout}
    </CodeBlock>
</CodeGroup>

### Dynamic credentials with `StorageOptionsProvider` <Badge color="green">Python-only</Badge>  
Use a storage options provider when credentials expire (for example, short-lived STS credentials). Pass any provider object that implements `fetch_storage_options()` with `storage_options_provider` on table operations such as `create_table` and `open_table`. In SDK versions that expose `StorageOptionsProvider`, you can subclass it directly.

If `fetch_storage_options()` returns `expires_at_millis`, LanceDB refreshes credentials before that timestamp. You can optionally set `refresh_offset_millis` (in milliseconds) to refresh earlier.

<CodeBlock language="Python" title="Refresh cloud credentials automatically" icon="python">
{PyStorageProviderRefresh}
</CodeBlock>

#### General object store options

| Key | Description |
| :-- | :-- |
| `allow_http` | Allow non-TLS connections. |
| `allow_invalid_certificates` | Skip certificate validation for TLS connections. |
| `connect_timeout` | Timeout for the connect phase. |
| `timeout` | Timeout for the full request. |
| `user_agent` | User agent string sent with requests. |
| `proxy_url` | Proxy URL to route requests through. |
| `proxy_ca_certificate` | PEM-formatted CA certificate for proxy connections. |
| `proxy_excludes` | Comma-separated hosts that bypass the proxy (domains or CIDR). |
| `download_retry_count` | Number of retries when downloading objects. |
| `client_max_retries` | Maximum retries for object-store client requests. |
| `client_retry_timeout` | Total retry timeout (seconds) for object-store client requests. |

<Info title="Option support varies by backend">
These are commonly used options. Cloud-specific keys (for example `region`, `endpoint`, `service_account`, and Azure credential keys) are backend-dependent and can be provided in `storage_options` as needed.
</Info>

## AWS S3

![](/static/assets/images/storage/aws.jpg)

Set `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and optionally `AWS_SESSION_TOKEN` as environment variables or pass them in `storage_options`. Region is optional for AWS but required for most S3-compatible stores.

Minimum permissions usually include `s3:PutObject`, `s3:GetObject`, `s3:DeleteObject`, `s3:ListBucket`, and `s3:GetBucketLocation` scoped to the relevant bucket/prefix.

### S3-compatible stores

<CodeGroup>
    <CodeBlock language="Python" title="Connect to an S3-compatible endpoint" icon="python">
    {PyStorageS3Minio}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to an S3-compatible endpoint" icon="square-js">
    {TsStorageS3Minio}
    </CodeBlock>
</CodeGroup>

If the endpoint is `http://` (common in local development), also set `ALLOW_HTTP=true` or pass `allow_http=True` in `storage_options`.

### S3 Express

<CodeGroup>
    <CodeBlock language="Python" title="Use an S3 Express One Zone bucket" icon="python">
    {PyStorageS3Express}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Use an S3 Express One Zone bucket" icon="square-js">
    {TsStorageS3Express}
    </CodeBlock>
</CodeGroup>

Consult AWS networking requirements for S3 Express before enabling.

### DynamoDB commit store for concurrent writes

S3 lacks atomic writes. To enable safe concurrent writers, use DynamoDB as a commit store by switching to the `s3+ddb` scheme and specifying the table name.

<CodeGroup>
    <CodeBlock language="Python" title="Enable DynamoDB-backed commits" icon="python">
    {PyStorageS3Ddb}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Enable DynamoDB-backed commits" icon="square-js">
    {TsStorageS3Ddb}
    </CodeBlock>
</CodeGroup>

Create the DynamoDB table with hash key `base_uri` (string) and range key `version` (number). Small provisioned throughput (for example `ReadCapacityUnits=1`, `WriteCapacityUnits=1`) is sufficient for coordination.

<Tip title="Clean up failed multipart uploads">
LanceDB aborts multipart uploads on graceful shutdown, but crashes can leave incomplete uploads. Add an S3 lifecycle rule to delete in-progress uploads after a few days.
</Tip>

## Google Cloud Storage

![](/static/assets/images/storage/gcp.jpg)

Provide credentials via `GOOGLE_SERVICE_ACCOUNT` (path to JSON) or include the path in `storage_options`. GCS defaults to HTTP/1; set `HTTP1_ONLY=false` if you need HTTP/2.

<CodeGroup>
    <CodeBlock language="Python" title="Connect with a service account" icon="python">
    {PyStorageGcsServiceAccount}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect with a service account" icon="square-js">
    {TsStorageGcsServiceAccount}
    </CodeBlock>
</CodeGroup>

## Azure Blob Storage

![](/static/assets/images/storage/azure.jpg)

Set `AZURE_STORAGE_ACCOUNT_NAME` and `AZURE_STORAGE_ACCOUNT_KEY` as environment variables, or pass them via `storage_options`.

<CodeGroup>
    <CodeBlock language="Python" title="Connect to Azure Blob Storage" icon="python">
    {PyStorageAzureAccount}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to Azure Blob Storage" icon="square-js">
    {TsStorageAzureAccount}
    </CodeBlock>
</CodeGroup>

Other supported keys include service principal credentials (`azure_client_id`, `azure_client_secret`, `azure_tenant_id`), SAS tokens, managed identities, and custom endpoints.

## Tigris Object Storage

![](/static/assets/images/storage/tigris.jpg)

Tigris exposes an S3-compatible API. Configure the endpoint and region:

<CodeGroup>
    <CodeBlock language="Python" title="Connect to Tigris Object Storage" icon="python">
    {PyStorageTigrisConnect}
    </CodeBlock>
    <CodeBlock language="TypeScript" title="Connect to Tigris Object Storage" icon="square-js">
    {TsStorageTigrisConnect}
    </CodeBlock>
</CodeGroup>

Environment variables `AWS_ENDPOINT=https://t3.storage.dev` and `AWS_DEFAULT_REGION=auto` achieve the same configuration.
